{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.Builder() \\\n",
    "    .appName(\"data_processor\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.elasticsearch:elasticsearch-spark-30_2.12:8.6.2\") \\\n",
    "    .master(\"local[4]\").getOrCreate()\n",
    "\n",
    "# df = spark.read.format('csv').option('header', 'true').load(\n",
    "#     'data/complaints_init.csv'\n",
    "# )\n",
    "\n",
    "df = spark.read.csv(\n",
    "    'data/training_init.csv',\n",
    "    header=True,\n",
    "    inferSchema=True    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/27 22:18:11 WARN RestClient: Could not verify server is Elasticsearch! ES-Hadoop will require server validation when connecting to an Elasticsearch cluster if that Elasticsearch cluster is v7.14 and up.\n",
      "23/11/27 22:18:11 WARN Resource: Detected type name in resource [test-kakfa-ingestion-flow/_doc]. Type names are deprecated and will be removed in a later release.\n",
      "23/11/27 22:18:11 WARN RestClient: Could not verify server is Elasticsearch! ES-Hadoop will require server validation when connecting to an Elasticsearch cluster if that Elasticsearch cluster is v7.14 and up.\n",
      "23/11/27 22:18:11 WARN Resource: Detected type name in resource [test-kakfa-ingestion-flow/_doc]. Type names are deprecated and will be removed in a later release.\n",
      "23/11/27 22:18:11 WARN Resource: Detected type name in resource [test-kakfa-ingestion-flow/_doc]. Type names are deprecated and will be removed in a later release.\n",
      "23/11/27 22:18:11 WARN Resource: Detected type name in resource [test-kakfa-ingestion-flow/_doc]. Type names are deprecated and will be removed in a later release.\n",
      "23/11/27 22:18:11 WARN Resource: Detected type name in resource [test-kakfa-ingestion-flow/_doc]. Type names are deprecated and will be removed in a later release.\n",
      "23/11/27 22:18:11 WARN Resource: Detected type name in resource [test-kakfa-ingestion-flow/_doc]. Type names are deprecated and will be removed in a later release.\n",
      "+--------------------+--------+----------+------+--------------------+---------------+\n",
      "|                date|    flag|        id|target|                text|           user|\n",
      "+--------------------+--------+----------+------+--------------------+---------------+\n",
      "|Sun May 17 10:41:...|NO_QUERY|1827430396|     4|@HowlinHounds tha...|   prestonreese|\n",
      "|Mon Jun 01 23:32:...|NO_QUERY|2000995177|     0|@patriciaco I kno...|   steph_davies|\n",
      "|Sun May 31 16:52:...|NO_QUERY|1985317134|     4|working till midn...|    Heyjude1970|\n",
      "|Thu May 14 01:21:...|NO_QUERY|1793097215|     4|@HumyoStorage spo...|     jamesmills|\n",
      "|Thu May 28 21:42:...|NO_QUERY|1956356911|     4|dippin 2 culver c...|      QBANVINNY|\n",
      "|Sun Jun 07 15:15:...|NO_QUERY|2069101945|     4|Thanks to all who...|      Kalibally|\n",
      "|Mon Jun 15 00:26:...|NO_QUERY|2175235569|     4|http://bit.ly/CYM...|custardthdragon|\n",
      "|Sun Jun 07 14:29:...|NO_QUERY|2068669007|     4|@VegasWill isn't ...|Lena_DISTRACTIA|\n",
      "|Mon Jun 15 14:53:...|NO_QUERY|2183723572|     4|@howlertwit Sound...|      jaybranch|\n",
      "|Sun Jun 07 12:53:...|NO_QUERY|2067749659|     4|I now officially ...|  absolut420Jen|\n",
      "|Tue Jun 02 05:42:...|NO_QUERY|2003020722|     4|Terminator was re...|        maylee_|\n",
      "|Tue Jun 02 00:49:...|NO_QUERY|2001418347|     0|Ha! I do homework...|      juliecaro|\n",
      "|Tue Jun 02 00:40:...|NO_QUERY|2001375838|     4|@maiasky james bo...|    hungryalien|\n",
      "|Mon Jun 15 01:41:...|NO_QUERY|2175682242|     4|Had a rough day, ...|    BuddyLuv345|\n",
      "|Wed Jun 17 15:59:...|NO_QUERY|2213896931|     0|@liamprescott wha...|     Jamesizzle|\n",
      "|Sat Jun 20 21:48:...|NO_QUERY|2262057437|     0|Hating that I hav...|        twebeck|\n",
      "|Sun Jun 07 14:57:...|NO_QUERY|2068926673|     4|@Jessicaveronica ...|      idontnow1|\n",
      "|Mon May 18 05:39:...|NO_QUERY|1835298065|     4|@louizah im so je...|  ThogoriwithaT|\n",
      "|Sun May 17 23:40:...|NO_QUERY|1833557860|     4|@peopleequal Than...|  FayLeMarquand|\n",
      "|Wed Jun 17 06:06:...|NO_QUERY|2206166337|     0|@shawnee_dj lmao,...|   chrishsleeps|\n",
      "+--------------------+--------+----------+------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = 'test-kakfa-ingestion-flow'\n",
    "df = spark.read.format(\"org.elasticsearch.spark.sql\").option(\"es.resource\", '%s/%s' % (INDEX_NAME, \"_doc\")) \\\n",
    "\n",
    "df.load().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSimulator:\n",
    "    def __init__(self, label_data_path):\n",
    "        # self.label_df = pd.read_csv(label_data_path)\n",
    "        self.label_df = spark.read.csv(\n",
    "            label_data_path,\n",
    "            header=True,\n",
    "            inferSchema=True\n",
    "        )\n",
    "\n",
    "    def annotate(self, ids):\n",
    "        return self.label_df.where(self.label_df['id'].isin(ids)).target\n",
    "        # return self.label_df[self.label_df.isin(ids)].target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = spark.read.csv('data/label.csv', header=True, inferSchema=True)\n",
    "df = spark.read.csv('data/training_init.csv', header=True, inferSchema=True).drop(\"target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------+--------------------+--------+--------------------+\n",
      "|        id|           user|target|                date|    flag|                text|\n",
      "+----------+---------------+------+--------------------+--------+--------------------+\n",
      "|1467810369|_TheSpecialOne_|     0|Mon Apr 06 22:19:...|NO_QUERY|@switchfoot http:...|\n",
      "|1467810672|  scotthamilton|     0|Mon Apr 06 22:19:...|NO_QUERY|is upset that he ...|\n",
      "|1467810917|       mattycus|     0|Mon Apr 06 22:19:...|NO_QUERY|@Kenichan I dived...|\n",
      "|1467811184|        ElleCTF|     0|Mon Apr 06 22:19:...|NO_QUERY|my whole body fee...|\n",
      "|1467811193|         Karoli|     0|Mon Apr 06 22:19:...|NO_QUERY|@nationwideclass ...|\n",
      "|1467811372|       joy_wolf|     0|Mon Apr 06 22:20:...|NO_QUERY|@Kwesidei not the...|\n",
      "|1467811592|        mybirch|     0|Mon Apr 06 22:20:...|NO_QUERY|         Need a hug |\n",
      "|1467811594|           coZZ|     0|Mon Apr 06 22:20:...|NO_QUERY|@LOLTrish hey  lo...|\n",
      "|1467811795|2Hood4Hollywood|     0|Mon Apr 06 22:20:...|NO_QUERY|@Tatiana_K nope t...|\n",
      "|1467812025|        mimismo|     0|Mon Apr 06 22:20:...|NO_QUERY|@twittera que me ...|\n",
      "|1467812416| erinx3leannexo|     0|Mon Apr 06 22:20:...|NO_QUERY|spring break in p...|\n",
      "|1467812579|   pardonlauren|     0|Mon Apr 06 22:20:...|NO_QUERY|I just re-pierced...|\n",
      "|1467812723|           TLeC|     0|Mon Apr 06 22:20:...|NO_QUERY|@caregiving I cou...|\n",
      "|1467812771|robrobbierobert|     0|Mon Apr 06 22:20:...|NO_QUERY|@octolinz16 It it...|\n",
      "|1467812784|    bayofwolves|     0|Mon Apr 06 22:20:...|NO_QUERY|@smarrison i woul...|\n",
      "|1467812799|     HairByJess|     0|Mon Apr 06 22:20:...|NO_QUERY|@iamjazzyfizzle I...|\n",
      "|1467812964| lovesongwriter|     0|Mon Apr 06 22:20:...|NO_QUERY|Hollis' death sce...|\n",
      "|1467813137|       armotley|     0|Mon Apr 06 22:20:...|NO_QUERY|about to file taxes |\n",
      "|1467813579|     starkissed|     0|Mon Apr 06 22:20:...|NO_QUERY|@LettyA ahh ive a...|\n",
      "|1467813782|      gi_gi_bee|     0|Mon Apr 06 22:20:...|NO_QUERY|@FakerPattyPattz ...|\n",
      "+----------+---------------+------+--------------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "lb.join(df, on=['id', 'user']).dropDuplicates(['id', 'user']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1467863684    2\n",
       "1467880442    2\n",
       "1468053611    2\n",
       "1468100580    2\n",
       "1468115720    2\n",
       "             ..\n",
       "2193278017    2\n",
       "2193403830    2\n",
       "2193428118    2\n",
       "2193451289    2\n",
       "2193576442    2\n",
       "Length: 1685, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldf_group[ldf_group>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10036"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.where(df['id'].isin(df.id)).show()\n",
    "\n",
    "\n",
    "df.join(lb, on=['id', 'user']).count()\n",
    "\n",
    "# lb.where(lb.id.isin(df.id)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "      .format( \"org.elasticsearch.spark.sql\" )\n",
    "      .option( \"es.nodes\",   \"localhost\" )\n",
    "      .option( \"es.port\",    5601     )\n",
    "    #   .option( \"es.net.ssl\", ssl      )\n",
    "      .option( \"es.nodes.wan.only\", \"true\" )\n",
    "      .load( f\"spark_index/doc\" )\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "from typing import Optional\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import HashingTF, IDF, StopWordsRemover, Tokenizer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark import keyword_only\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from pyspark.sql.dataframe import DataFrame \n",
    "from ml.slang_words import slang\n",
    "\n",
    "\n",
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad',\n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed',\n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink',\n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "\n",
    "class Preprocessor(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    input_col = Param(Params._dummy(), \"input_col\", \"input column name.\", typeConverter=TypeConverters.toString)\n",
    "    output_col = Param(Params._dummy(), \"output_col\", \"output column name.\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        super(Preprocessor, self).__init__()\n",
    "        self._setDefault(input_col=None, output_col=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.set_params(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def set_params(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "        \n",
    "    def get_input_col(self):\n",
    "        return self.getOrDefault(self.input_col)\n",
    "    \n",
    "    def get_output_col(self):\n",
    "        return self.getOrDefault(self.output_col)\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        pattern = re.compile('<.*?>')\n",
    "        return pattern.sub(r'', text)\n",
    "    \n",
    "    def remove_url(self, text):\n",
    "        pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        return pattern.sub(r'', text)\n",
    "    \n",
    "    def handle_emoji(self, text):\n",
    "        for emoji in emojis.keys():\n",
    "            text = text.replace(emoji, \"EMOJI\" + emojis[emoji])\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def chat_conversion(self, text):\n",
    "        new_text = []\n",
    "        for w in text.split():\n",
    "            if w.upper() in slang:\n",
    "                new_text.append(slang[w.upper()])\n",
    "            else:\n",
    "                new_text.append(w)\n",
    "        return \" \".join(new_text)\n",
    "    \n",
    "    def remove_punc(self, text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    def lowercase(self, text):\n",
    "        return text.lower()\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        text = self.remove_html_tags(text)\n",
    "        text = self.remove_url(text)\n",
    "        text = self.handle_emoji(text)\n",
    "        text = self.chat_conversion(text)\n",
    "        text = self.remove_punc(text)\n",
    "        text = self.lowercase(text)\n",
    "\n",
    "        return text \n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        input_col = self.get_input_col()\n",
    "        output_col = self.get_output_col()\n",
    "\n",
    "        transform_udf = F.udf(self.preprocess, StringType())\n",
    "\n",
    "        return df.withColumn(output_col, transform_udf(input_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(df):\n",
    "  preprocessor = Preprocessor(input_col=\"text\", output_col=\"text\")\n",
    "  # Tokenize into words\n",
    "  tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokenized\")\n",
    "  # Remove stopwords\n",
    "  remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "  # Compute term frequencies and hash into buckets\n",
    "  hashing_tf = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"hashed\",\\\n",
    "    numFeatures=1000)\n",
    "  # Convert to TF-IDF\n",
    "  idf = IDF(inputCol=hashing_tf.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "\n",
    "  pipeline = Pipeline(stages=[preprocessor, tokenizer, remover, hashing_tf, idf])\n",
    "  pipeline_model = pipeline.fit(df)\n",
    "\n",
    "  tolist_udf = F.udf(\n",
    "      lambda v: v.toArray().tolist(), \n",
    "      ArrayType(FloatType())\n",
    "  )\n",
    "  return pipeline_model.transform(df).\\\n",
    "    withColumn(\"features\", tolist_udf(\"features\")).cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "class LabelerSimulator(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    input_col = Param(Params._dummy(), \"input_col\", \"input column name.\", typeConverter=TypeConverters.toString)\n",
    "    output_col = Param(Params._dummy(), \"output_col\", \"output column name.\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        super(Preprocessor, self).__init__()\n",
    "        self._setDefault(input_col=None, output_col=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.set_params(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def set_params(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "        \n",
    "    def get_input_col(self):\n",
    "        return self.getOrDefault(self.input_col)\n",
    "    \n",
    "    def get_output_col(self):\n",
    "        return self.getOrDefault(self.output_col)\n",
    "\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        input_col = self.get_input_col()\n",
    "        output_col = self.get_output_col()\n",
    "\n",
    "        transform_udf = F.udf(self.preprocess, StringType())\n",
    "\n",
    "        return df.withColumn(output_col, transform_udf(input_col))\n",
    "\n",
    "\n",
    "# init dataset\n",
    "df_tr = feature_extraction(df)\n",
    "# X = df_tr['feature']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    'data/training_init.csv',\n",
    "    header=True,\n",
    "    inferSchema=True    \n",
    ")\n",
    "\n",
    "\n",
    "labels = spark.read.csv(\"data/label.csv\",header=True,\n",
    "    inferSchema=True    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, f1_score\n",
    "from hyperopt import fmin, hp, tpe\n",
    "from hyperopt import SparkTrials, STATUS_OK\n",
    "\n",
    "# Model\n",
    "def build_test_train_split(df, val_size, test_size):\n",
    "  # modAL needs numpy arrays; this converts M arrays of N-d arrays into MxN 2D array\n",
    "  X = np.stack(df['features'].to_numpy())\n",
    "  y = df['target'].to_numpy()\n",
    "  \n",
    "  (X_train, X2, y_train, y2) = train_test_split(\n",
    "    X, y, \n",
    "    test_size = val_size + test_size, \n",
    "    stratify = y,\n",
    "    random_state = 7\n",
    "  )\n",
    "  \n",
    "  (X_val, X_test, y_val, y_test) = train_test_split(\n",
    "    X2, y2, \n",
    "    test_size = test_size/(val_size + test_size),\n",
    "    stratify = y2,\n",
    "    random_state = 7\n",
    "  )\n",
    "  return (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "# Core function to train a model given train set and params\n",
    "def train_model(params, X_train, y_train):\n",
    "  lr = LogisticRegression(solver='liblinear', max_iter=1000,\\\n",
    "                          penalty=params['penalty'], C=params['C'], random_state=7)\n",
    "  return lr.fit(X_train, y_train)\n",
    "\n",
    "# Use hyperopt to select a best model, given train/validation sets\n",
    "def find_best_lr_model(X_train, X_val, y_train, y_val):\n",
    "  # Wraps core modeling function to evaluate and return results for hyperopt\n",
    "  def train_model_fmin(params):\n",
    "    lr = train_model(params, X_train, y_train)\n",
    "    loss = log_loss(y_val, lr.predict_proba(X_val))\n",
    "    accuracy = accuracy_score(y_val, lr.predict(X_val))\n",
    "    f1 = f1_score(y_val, lr.predict(X_val), pos_label=4)\n",
    "    # supplement auto logging in mlflow with f1 and accuracy\n",
    "    mlflow.log_metric('f1', f1)\n",
    "    mlflow.log_metric('accuracy', accuracy)\n",
    "    return {\n",
    "      'status': STATUS_OK, \n",
    "      'loss': loss, \n",
    "      'accuracy': accuracy, \n",
    "      'f1': f1\n",
    "    }\n",
    "\n",
    "  penalties = ['l1', 'l2']\n",
    "  search_space = {\n",
    "    'C': hp.loguniform('C', -6, 1),\n",
    "    'penalty': hp.choice('penalty', penalties)\n",
    "  }\n",
    "\n",
    "  best_params = fmin(fn=train_model_fmin,\n",
    "                     space=search_space,\n",
    "                     algo=tpe.suggest,\n",
    "                     max_evals=32,\n",
    "                     trials=SparkTrials(parallelism=4),\n",
    "                     rstate=np.random.default_rng(7))\n",
    "  # Need to translate this back from 0/1 in output to be used again as input\n",
    "  best_params['penalty'] = penalties[best_params['penalty']]\n",
    "  # Train final model on train + validation sets\n",
    "  final_model = train_model(best_params,\\\n",
    "                            np.concatenate([X_train, X_val]),\\\n",
    "                            np.concatenate([y_train, y_val]))\n",
    "  return (best_params, final_model)\n",
    "\n",
    "def log_and_eval_model(best_model, best_params, X_test, y_test):\n",
    "  with mlflow.start_run():\n",
    "    accuracy = accuracy_score(y_test, best_model.predict(X_test))\n",
    "    f1 = f1_score(y_test, best_model.predict(X_test), pos_label=4)\n",
    "    loss = log_loss(y_test, best_model.predict_proba(X_test))\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metrics({'accuracy': accuracy, 'log_loss': loss})\n",
    "    mlflow.sklearn.log_model(best_model, \"model\")\n",
    "    return (accuracy, f1, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "print(f1_score(\n",
    "    y_test, best_model.predict(X_test), pos_label=4\n",
    "))\n",
    "print(classification_report(\n",
    "    y_test, best_model.predict(X_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(X_train, X_val, X_test, y_train, y_val, y_test) = build_test_train_split(\n",
    "    df_tr.toPandas(), 0.1, 0.1\n",
    ")\n",
    "(best_params, best_model) = find_best_lr_model(X_train, X_val, y_train, y_val)\n",
    "(accuracy, f1, loss) = log_and_eval_model(best_model, best_params, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_and_eval_model(best_model, best_params, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duy_bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
